{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a24163f-3ad0-4bac-89c7-9afb16861169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_string_by_newline(s: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    将字符串从第1000个字符开始，按'\\n'、'\\\\n'或字符串结尾进行划分，返回字符串数组\n",
    "    \n",
    "    参数:\n",
    "        s: 要划分的原始字符串\n",
    "        \n",
    "    返回:\n",
    "        list[str]: 划分后的字符串列表\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    start = 0\n",
    "    chunk_size = 1000  # 从第1000个字符开始寻找分割点\n",
    "    \n",
    "    while start < len(s):\n",
    "        # 计算当前 chunk 的结束位置（从start开始的第1000个字符）\n",
    "        end_candidate = start + chunk_size\n",
    "        \n",
    "        # 如果剩余字符不足1000，直接取到结尾\n",
    "        if end_candidate >= len(s):\n",
    "            result.append(s[start:])\n",
    "            break\n",
    "        \n",
    "        # 从第1000个字符位置开始寻找分割点\n",
    "        # 搜索范围：end_candidate 到字符串结尾（避免漏检）\n",
    "        search_start = end_candidate\n",
    "        split_pos = -1\n",
    "        \n",
    "        # 优先寻找'\\n'（实际换行符）\n",
    "        split_pos = s.find('\\n', search_start)\n",
    "        \n",
    "        # 如果没找到'\\n'，寻找'\\\\n'（转义的换行符）\n",
    "        if split_pos == -1:\n",
    "            split_pos = s.find('\\\\n', search_start)\n",
    "            # 找到'\\\\n'时，分割位置需要包含两个字符\n",
    "            if split_pos != -1:\n",
    "                split_pos += 2  # 指向'\\\\n'之后的位置\n",
    "        \n",
    "        # 如果两种换行符都没找到，分割到字符串结尾\n",
    "        if split_pos == -1:\n",
    "            split_pos = len(s)\n",
    "        \n",
    "        # 将当前 chunk 加入结果列表\n",
    "        result.append(s[start:split_pos])\n",
    "        \n",
    "        # 更新下一个 chunk 的起始位置\n",
    "        start = split_pos\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0eab5a-4fce-41a8-977e-2bf4bef44b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "# 文件路径配置\n",
    "input_path = r\"C:\\Users\\12860\\Desktop\\科研\\crosscodeeval\\python\\modified_line_completion.json\"\n",
    "output_path = r\"C:\\Users\\12860\\Desktop\\科研\\crosscodeeval\\python\\modified_line_completion_split.json\"\n",
    "\n",
    "# 检查输入文件是否存在\n",
    "if not os.path.exists(input_path):\n",
    "    raise FileNotFoundError(f\"输入文件未找到：{input_path}\")\n",
    "\n",
    "# 读取原始数据集\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "print(f\"✅ 成功加载数据集，共 {len(dataset)} 条记录\")\n",
    "\n",
    "# 处理每条记录\n",
    "processed_dataset = []\n",
    "for item in tqdm(dataset, desc=\"处理进度\"):\n",
    "    # 复制原始字段（保留completion）\n",
    "    processed_item = {\n",
    "        \"completion\": item.get(\"completion\", \"\")\n",
    "    }\n",
    "    \n",
    "    # 处理prefix划分\n",
    "    prefix = item.get(\"prefix\", \"\")\n",
    "    prefix_splits = split_string_by_newline(prefix)\n",
    "    for i, split in enumerate(prefix_splits, 1):\n",
    "        processed_item[f\"prefix_split_{i}\"] = split\n",
    "    \n",
    "    # 处理suffix划分\n",
    "    suffix = item.get(\"suffix\", \"\")\n",
    "    suffix_splits = split_string_by_newline(suffix)\n",
    "    for i, split in enumerate(suffix_splits, 1):\n",
    "        processed_item[f\"suffix_split_{i}\"] = split\n",
    "    \n",
    "    processed_dataset.append(processed_item)\n",
    "\n",
    "# 保存处理后的数据集\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(processed_dataset, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 验证结果\n",
    "print(f\"\\n✅ 数据集处理完成，已保存至：{output_path}\")\n",
    "\n",
    "# 打印第一条记录的结构，确认划分结果\n",
    "if processed_dataset:\n",
    "    first_item = processed_dataset[0]\n",
    "    print(\"\\n第一条记录的划分结果（字段列表）：\")\n",
    "    print(list(first_item.keys()))\n",
    "    \n",
    "    # 统计prefix和suffix的划分数量\n",
    "    prefix_split_count = sum(1 for key in first_item if key.startswith(\"prefix_split_\"))\n",
    "    suffix_split_count = sum(1 for key in first_item if key.startswith(\"suffix_split_\"))\n",
    "    print(f\"\\n第一条记录中：\")\n",
    "    print(f\"  prefix被划分为 {prefix_split_count} 部分\")\n",
    "    print(f\"  suffix被划分为 {suffix_split_count} 部分\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481be36f-79eb-4d28-8247-da417694bb90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b8b2ad-c622-4c5c-8943-c4c2312f1c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_trunc_string(s: str) -> str:\n",
    "    \"\"\"\n",
    "    从字符串末端反向计数1000个字符，向前寻找第一个'\\n'或'\\\\n'，\n",
    "    返回从分割点到末端的子字符串，并去除开头的换行符和空格\n",
    "    \"\"\"\n",
    "    if len(s) <= 1000:\n",
    "        truncated = s\n",
    "    else:\n",
    "        start_pos = len(s) - 1000\n",
    "        # 优先寻找'\\n'\n",
    "        split_pos = s.rfind('\\n', 0, start_pos + 1)\n",
    "        \n",
    "        # 寻找'\\\\n'\n",
    "        if split_pos == -1:\n",
    "            split_pos = s.rfind('\\\\n', 0, start_pos + 1)\n",
    "            if split_pos != -1:\n",
    "                split_pos += 2  # 从转义义换行符后开始取\n",
    "        \n",
    "        # 未找到换行符则从start_pos开始取\n",
    "        if split_pos == -1:\n",
    "            split_pos = start_pos\n",
    "        \n",
    "        truncated = s[split_pos:]\n",
    "    \n",
    "    # 去除除开头的换行符(\\n)、转义换行符(\\\\n)和空格\n",
    "    # 先处理转义换行符\n",
    "    while truncated.startswith(('\\\\n', '\\n', ' ')):\n",
    "        if truncated.startswith('\\\\n'):\n",
    "            truncated = truncated[2:]  # 移除\\\\n\n",
    "        elif truncated.startswith('\\n'):\n",
    "            truncated = truncated[1:]  # 移除\\n\n",
    "        elif truncated.startswith(' '):\n",
    "            truncated = truncated.lstrip(' ')  # 移除空格\n",
    "    \n",
    "    return truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa4e96d-74e6-43e1-b41e-20890d4b844e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11908870-d645-4cc9-ad6f-7a6807f77419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文件路径配置\n",
    "input_path = r\"C:\\Users\\12860\\Desktop\\科研\\crosscodeeval\\python\\modified_line_completion.json\"\n",
    "output_path = r\"C:\\Users\\12860\\Desktop\\科研\\crosscodeeval\\python\\modified_line_completion_trunc.json\"\n",
    "\n",
    "# 检查输入文件是否存在\n",
    "if not os.path.exists(input_path):\n",
    "    raise FileNotFoundError(f\"输入文件未找到：{input_path}\")\n",
    "\n",
    "# 读取原始数据集\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "print(f\"✅ 成功加载数据集，共 {len(dataset)} 条记录\")\n",
    "\n",
    "# 处理每条记录\n",
    "processed_dataset = []\n",
    "for item in tqdm(dataset, desc=\"处理进度\"):\n",
    "    # 对prefix和suffix进行截断处理\n",
    "    processed_item = {\n",
    "        \"prefix_trunc_1000\": reverse_trunc_string(item.get(\"prefix\", \"\")),\n",
    "        \"suffix_trunc_1000\": reverse_trunc_string(item.get(\"suffix\", \"\")),\n",
    "        \"completion\": item.get(\"completion\", \"\")  # 保持completion不变\n",
    "    }\n",
    "    processed_dataset.append(processed_item)\n",
    "\n",
    "# 保存处理后的数据集\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(processed_dataset, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 验证结果\n",
    "print(f\"\\n✅ 数据集处理完成，已保存至：{output_path}\")\n",
    "\n",
    "# 打印第一条记录的截断前后长度对比\n",
    "if processed_dataset and dataset:\n",
    "    first_original = dataset[0]\n",
    "    first_processed = processed_dataset[0]\n",
    "    \n",
    "    print(\"\\n第一条记录截断前后长度对比：\")\n",
    "    print(f\"  原始prefix长度：{len(first_original.get('prefix', ''))} → 截断后：{len(first_processed['prefix_trunc_1000'])}\")\n",
    "    print(f\"  原始suffix长度：{len(first_original.get('suffix', ''))} → 截断后：{len(first_processed['suffix_trunc_1000'])}\")\n",
    "    print(f\"  completion长度：{len(first_processed['completion'])}（未截断）\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756ddef8-2e3a-478d-bb70-7193b02db79c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b2a2160-8cbb-44b6-8d5c-f7e8e3339c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 成功加载数据集，共 272 条记录\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "清理代码块标记: 100%|██████████| 272/272 [00:00<00:00, 200783.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 清理完成！结果保存至：\n",
      "   C:\\Users\\12860\\Desktop\\科研\\crosscodeeval\\python\\temp_cleaned.json\n",
      "\n",
      "prefix_split_1 清理示例（前30字符）：\n",
      "import asyncio\n",
      "import websocke...\n",
      "\n",
      "suffix_split_1 清理示例（前30字符）：\n",
      "next_token = <LibFunc->(use ge...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clean_code_blocks() -> None:\n",
    "    \"\"\"\n",
    "    处理数据集中所有对象，对prefix_split_*和suffix_split_*字段检查并移除\n",
    "    前缀(\"```\"或\"```python\")和后缀(\"```\")，保持与annotate函数一致的遍历风格\n",
    "    \"\"\"\n",
    "    # 文件路径配置\n",
    "    input_path = r\"C:\\Users\\12860\\Desktop\\科研\\crosscodeeval\\python\\temp.json\"\n",
    "    output_path = r\"C:\\Users\\12860\\Desktop\\科研\\crosscodeeval\\python\\temp_cleaned.json\"\n",
    "    \n",
    "    # 检查输入文件是否存在\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"输入文件未找到：{input_path}\")\n",
    "    \n",
    "    # 读取数据集（列表中的每个元素是字典）\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        dataset = json.load(f)\n",
    "    \n",
    "    # 验证数据集格式\n",
    "    if not isinstance(dataset, list):\n",
    "        raise TypeError(\"数据集应为列表格式，每个元素应为字典\")\n",
    "    \n",
    "    print(f\"✅ 成功加载数据集，共 {len(dataset)} 条记录\")\n",
    "    \n",
    "    # 处理所有对象\n",
    "    processed_dataset = []\n",
    "    for item in tqdm(dataset, desc=\"清理代码块标记\"):\n",
    "        # 复制原始对象（保留所有字段）\n",
    "        cleaned_item = {}\n",
    "        \n",
    "        # 处理所有prefix_split字段\n",
    "        prefix_fields = [key for key in item if key.startswith(\"prefix_split_\")]\n",
    "        for field in prefix_fields:\n",
    "            content = item[field]\n",
    "            # 检查并移除前缀\n",
    "            if content.startswith(\"```python\"):\n",
    "                content = content[len(\"```python\"):]\n",
    "            elif content.startswith(\"```\"):\n",
    "                content = content[len(\"```\"):]\n",
    "            # 检查并移除后缀\n",
    "            if content.endswith(\"```\"):\n",
    "                content = content[:-len(\"```\")]\n",
    "            # 去除首尾空白\n",
    "            cleaned_item[field] = content\n",
    "        \n",
    "        # 处理所有suffix_split字段\n",
    "        suffix_fields = [key for key in item if key.startswith(\"suffix_split_\")]\n",
    "        for field in suffix_fields:\n",
    "            content = item[field]\n",
    "            # 检查并移除前缀\n",
    "            if content.startswith(\"```python\"):\n",
    "                content = content[len(\"```python\"):]\n",
    "            elif content.startswith(\"```\"):\n",
    "                content = content[len(\"```\"):]\n",
    "            # 检查并移除后缀\n",
    "            if content.endswith(\"```\"):\n",
    "                content = content[:-len(\"```\")]\n",
    "            # 去除首尾空白\n",
    "            cleaned_item[field] = content\n",
    "        \n",
    "        # 保留其他字段（如completion）\n",
    "        other_fields = [key for key in item \n",
    "                       if not key.startswith(\"prefix_split_\") \n",
    "                       and not key.startswith(\"suffix_split_\")]\n",
    "        for field in other_fields:\n",
    "            cleaned_item[field] = item[field]\n",
    "        \n",
    "        processed_dataset.append(cleaned_item)\n",
    "    \n",
    "    # 保存清理后的数据集\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(processed_dataset, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\n✅ 清理完成！结果保存至：\")\n",
    "    print(f\"   {output_path}\")\n",
    "    \n",
    "    # 验证清理结果\n",
    "    if processed_dataset:\n",
    "        first_item = processed_dataset[0]\n",
    "        # 查找示例字段\n",
    "        sample_prefix = next((key for key in first_item if key.startswith(\"prefix_split_\")), None)\n",
    "        sample_suffix = next((key for key in first_item if key.startswith(\"suffix_split_\")), None)\n",
    "        \n",
    "        if sample_prefix:\n",
    "            print(f\"\\n{sample_prefix} 清理示例（前30字符）：\")\n",
    "            print(f\"{first_item[sample_prefix][:30]}...\")\n",
    "        if sample_suffix:\n",
    "            print(f\"\\n{sample_suffix} 清理示例（前30字符）：\")\n",
    "            print(f\"{first_item[sample_suffix][:30]}...\")\n",
    "\n",
    "\n",
    "# 执行清理\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        clean_code_blocks()\n",
    "    except Exception as e:\n",
    "        print(f\"处理过程中发生错误：{str(e)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
